{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911e43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490b17a",
   "metadata": {},
   "source": [
    "# Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df05d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b651844431fb4b6fa62e87c93e86c5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427e9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data to use for training and testing\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96fda84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff701d6",
   "metadata": {},
   "source": [
    "# Preparing the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f0f170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.300641105303069\n",
      "Training loss: 2.290288946512715\n",
      "Training loss: 2.271466397873276\n",
      "Training loss: 2.2197420307437477\n",
      "Training loss: 2.1250560998611743\n",
      "Training loss: 2.029706495039908\n",
      "Training loss: 1.9557816817632416\n",
      "Training loss: 1.9033591886005743\n",
      "Training loss: 1.8582003641006586\n",
      "Training loss: 1.814921573604769\n",
      "Training loss: 1.7723704664908406\n",
      "Training loss: 1.7323967498891495\n",
      "Training loss: 1.6944600991580798\n",
      "Training loss: 1.6576210769546\n",
      "Training loss: 1.6234018253853253\n",
      "Training loss: 1.591349217135583\n",
      "Training loss: 1.5616243899325886\n",
      "Training loss: 1.5313827770445354\n",
      "Training loss: 1.5031048181416737\n",
      "Training loss: 1.4744850044970013\n",
      "Training loss: 1.447133968858158\n",
      "Training loss: 1.4191726613837434\n",
      "Training loss: 1.3922113302113759\n",
      "Training loss: 1.3657440529455005\n",
      "Training loss: 1.3387598922795347\n",
      "Training loss: 1.313007937977686\n",
      "Training loss: 1.2872108860546365\n",
      "Training loss: 1.2614376971788723\n",
      "Training loss: 1.2365180690727575\n",
      "Training loss: 1.2108186398015912\n",
      "Training loss: 1.184733098730102\n",
      "Training loss: 1.157194490780306\n",
      "Training loss: 1.133098399852548\n",
      "Training loss: 1.1080551014836793\n",
      "Training loss: 1.083337835796044\n",
      "Training loss: 1.0582447246364926\n",
      "Training loss: 1.0358118394299236\n",
      "Training loss: 1.0124431821086524\n",
      "Training loss: 0.9922055470211731\n",
      "Training loss: 0.9682133986669428\n",
      "Training loss: 0.9475339326407294\n",
      "Training loss: 0.9203274468784137\n",
      "Training loss: 0.9002269340293182\n",
      "Training loss: 0.8787590928394776\n",
      "Training loss: 0.8639036195204995\n",
      "Training loss: 0.8360069411642411\n",
      "Training loss: 0.8224540919903904\n",
      "Training loss: 0.7977350521880342\n",
      "Training loss: 0.7779880659964383\n",
      "Training loss: 0.7621080829283161\n",
      "Training loss: 0.7299585551252146\n",
      "Training loss: 0.7226348643946221\n",
      "Training loss: 0.6896469532071478\n",
      "Training loss: 0.6784600015643918\n",
      "Training loss: 0.6627350821900551\n",
      "Training loss: 0.6428364676511501\n",
      "Training loss: 0.635234480433147\n",
      "Training loss: 0.607829239057458\n",
      "Training loss: 0.5780297833330491\n",
      "Training loss: 0.5660093353151361\n",
      "Training loss: 0.5665984127070288\n",
      "Training loss: 0.5352589207537034\n",
      "Training loss: 0.5107993825393564\n",
      "Training loss: 0.5140230968746993\n",
      "Training loss: 0.48714523394699294\n",
      "Training loss: 0.4706412872771168\n",
      "Training loss: 0.4865974912710507\n",
      "Training loss: 0.4204946601825297\n",
      "Training loss: 0.43054302846607956\n",
      "Training loss: 0.4015316198129788\n",
      "Training loss: 0.40453119511189667\n",
      "Training loss: 0.4089745943960936\n",
      "Training loss: 0.3616617976895074\n",
      "Training loss: 0.3415863616749301\n",
      "Training loss: 0.3732919118383809\n",
      "Training loss: 0.3123234008889064\n",
      "Training loss: 0.3010963030121363\n",
      "Training loss: 0.3346891266381954\n",
      "Training loss: 0.29077454769740935\n",
      "Training loss: 0.286051713225558\n",
      "Training loss: 0.29579753946046083\n",
      "Training loss: 0.22049201969676616\n",
      "Training loss: 0.26072333615911586\n",
      "Training loss: 0.27304300549142346\n",
      "Training loss: 0.23123234892478378\n",
      "Training loss: 0.22896639036629207\n",
      "Training loss: 0.18766764018332105\n",
      "Training loss: 0.2684662670535428\n",
      "Training loss: 0.18109579463405986\n",
      "Training loss: 0.1396871694194539\n",
      "Training loss: 0.19362833611000224\n",
      "Training loss: 0.18668037801123488\n",
      "Training loss: 0.1434392815841662\n",
      "Training loss: 0.11354937017573725\n",
      "Training loss: 0.19609490779164196\n",
      "Training loss: 0.14476875065470976\n",
      "Training loss: 0.12185855181244633\n",
      "Training loss: 0.14713506124046682\n",
      "Training loss: 0.16294130044834465\n",
      "Training loss: 0.06142769888986636\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(3072,1536),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(1536, 768),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(768,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51d5eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_t, labels_t = next(iter(testloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b02cb266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f6a7cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_t=images_t.view(images_t.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ac742c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3072])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d65cb9",
   "metadata": {},
   "source": [
    "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are precision and recall and top-5 error rate. We'll focus on accuracy here. First I'll do a forward pass with one batch from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9efef054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# images_t, labels_t = next(iter(testloader))\n",
    "# images_t=images_t.view(images_t.shape[0], -1)\n",
    "# Get the class probabilities\n",
    "ps = torch.exp(model(images_t))\n",
    "# # Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
    "print(ps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1b910",
   "metadata": {},
   "source": [
    "With the probabilities, we can get the most likely class using the ps.topk method. This returns the  ð‘˜  highest values. Since we just want the most likely class, we can use ps.topk(1). This returns a tuple of the top- ð‘˜  values and the top- ð‘˜  indices. If the highest value is the fifth element, we'll get back 4 as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dd294eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4],\n",
      "        [5],\n",
      "        [3],\n",
      "        [1],\n",
      "        [9],\n",
      "        [4],\n",
      "        [3],\n",
      "        [6],\n",
      "        [8],\n",
      "        [4]]) 64\n"
     ]
    }
   ],
   "source": [
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# Look at the most likely classes for the first 10 examples\n",
    "print(top_class[:10,:],len(top_class[:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413cda58",
   "metadata": {},
   "source": [
    "Equals is comparing the one element in each row of top_class with each element in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae659595",
   "metadata": {},
   "outputs": [],
   "source": [
    "equals = top_class == labels_t.view(*top_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ca89e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.5625%\n"
     ]
    }
   ],
   "source": [
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f'Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fff8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
